# -*- coding: utf-8 -*-
"""Smart-SDLC

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uvWwZIfpnjwRTbDludqFjeUpSzMUj-Ep
"""

# SmartSDLC - AI-Enhanced Software Development Lifecycle
# Backend Implementation for Google Colab GPU

# Install required packages
!pip install flask flask-cors PyMuPDF transformers torch accelerate huggingface_hub requests langchain pyngrok

import os
import re
import json
import fitz  # PyMuPDF
import torch
from flask import Flask, request, jsonify, render_template_string
from flask_cors import CORS
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from huggingface_hub import login
import requests
from pyngrok import ngrok
import threading
import time

# Configuration
class Config:
    HUGGINGFACE_API_KEY = "API_KEY"  # Replace with your API key
    MODEL_NAME = "API_KEY"
    MAX_LENGTH = 512
    TEMPERATURE = 0.7

# Initialize Flask app
app = Flask(__name__)
CORS(app)

# Global variables for model
tokenizer = None
model = None
device = None

def setup_gpu_and_model():
    """Setup GPU and load the model"""
    global tokenizer, model, device

    # Check GPU availability
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")

    # Login to Hugging Face
    login(token=Config.HUGGINGFACE_API_KEY)

    # Load tokenizer and model
    print("Loading tokenizer and model...")
    tokenizer = AutoTokenizer.from_pretrained(Config.MODEL_NAME)
    model = AutoModelForCausalLM.from_pretrained(
        Config.MODEL_NAME,
        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
        device_map="auto" if torch.cuda.is_available() else None,
        trust_remote_code=True
    )

    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    print("Model loaded successfully!")

def generate_response(prompt, max_length=Config.MAX_LENGTH):
    """Generate response using the loaded model"""
    try:
        inputs = tokenizer.encode(prompt, return_tensors="pt", truncation=True, max_length=512)
        inputs = inputs.to(device)

        with torch.no_grad():
            outputs = model.generate(
                inputs,
                max_length=max_length,
                temperature=Config.TEMPERATURE,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id,
                num_return_sequences=1
            )

        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        # Remove the input prompt from response
        response = response[len(tokenizer.decode(inputs[0], skip_special_tokens=True)):].strip()
        return response
    except Exception as e:
        print(f"Error generating response: {e}")
        return "Sorry, I couldn't generate a response at this time."

def extract_pdf_text(pdf_file):
    """Extract text from PDF file"""
    try:
        doc = fitz.open(stream=pdf_file.read(), filetype="pdf")
        text = ""
        for page in doc:
            text += page.get_text()
        doc.close()
        return text
    except Exception as e:
        return f"Error extracting PDF: {str(e)}"

def classify_requirements(text):
    """Classify requirements into SDLC phases"""
    sentences = re.split(r'[.!?]+', text)
    sentences = [s.strip() for s in sentences if s.strip()]

    classifications = []

    for sentence in sentences[:10]:  # Limit to avoid too long processing
        prompt = f"""
        Classify the following requirement sentence into one of these SDLC phases:
        - Requirements
        - Design
        - Development
        - Testing
        - Deployment

        Sentence: "{sentence}"

        Classification:"""

        classification = generate_response(prompt, max_length=100)

        # Extract phase from response
        phases = ["Requirements", "Design", "Development", "Testing", "Deployment"]
        detected_phase = "Requirements"  # default
        for phase in phases:
            if phase.lower() in classification.lower():
                detected_phase = phase
                break

        classifications.append({
            "sentence": sentence,
            "phase": detected_phase
        })

    return classifications

# API Routes

@app.route('/')
def home():
    return render_template_string("""
    <!DOCTYPE html>
    <html>
    <head>
        <title>SmartSDLC API</title>
    </head>
    <body>
        <h1>SmartSDLC - AI-Enhanced Software Development Lifecycle</h1>
        <p>Backend API is running successfully!</p>
        <h2>Available Endpoints:</h2>
        <ul>
            <li>POST /api/upload-requirements - Upload and classify requirements</li>
            <li>POST /api/generate-code - Generate code from prompts</li>
            <li>POST /api/fix-bug - Fix bugs in code</li>
            <li>POST /api/generate-tests - Generate test cases</li>
            <li>POST /api/summarize-code - Summarize code</li>
            <li>POST /api/chat - Chat with AI assistant</li>
        </ul>
    </body>
    </html>
    """)

@app.route('/api/upload-requirements', methods=['POST'])
def upload_requirements():
    """Scenario 1: Requirement Upload and Classification"""
    try:
        if 'file' not in request.files:
            return jsonify({'error': 'No file uploaded'}), 400

        file = request.files['file']
        if file.filename == '':
            return jsonify({'error': 'No file selected'}), 400

        # Extract text from PDF
        text = extract_pdf_text(file)

        # Classify requirements
        classifications = classify_requirements(text)

        # Group by phase
        grouped = {}
        for item in classifications:
            phase = item['phase']
            if phase not in grouped:
                grouped[phase] = []
            grouped[phase].append(item['sentence'])

        return jsonify({
            'success': True,
            'original_text': text[:500] + '...' if len(text) > 500 else text,
            'classifications': classifications,
            'grouped_by_phase': grouped
        })

    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/generate-code', methods=['POST'])
def generate_code():
    """Scenario 2: AI Code Generator"""
    try:
        data = request.get_json()
        prompt = data.get('prompt', '')
        language = data.get('language', 'python')

        code_prompt = f"""
        Generate {language} code for the following requirement:

        {prompt}

        Please provide clean, production-ready code with comments:

        ```{language}
        """

        generated_code = generate_response(code_prompt, max_length=1024)

        return jsonify({
            'success': True,
            'prompt': prompt,
            'language': language,
            'generated_code': generated_code
        })

    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/fix-bug', methods=['POST'])
def fix_bug():
    """Scenario 3: Bug Fixer"""
    try:
        data = request.get_json()
        buggy_code = data.get('code', '')
        language = data.get('language', 'python')

        fix_prompt = f"""
        Fix the bugs in the following {language} code and provide the corrected version:

        Original Code:
        ```{language}
        {buggy_code}
        ```

        Fixed Code:
        ```{language}
        """

        fixed_code = generate_response(fix_prompt, max_length=1024)

        return jsonify({
            'success': True,
            'original_code': buggy_code,
            'fixed_code': fixed_code,
            'language': language
        })

    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/generate-tests', methods=['POST'])
def generate_tests():
    """Scenario 4: Test Case Generator"""
    try:
        data = request.get_json()
        code_or_requirement = data.get('input', '')
        framework = data.get('framework', 'unittest')

        test_prompt = f"""
        Generate {framework} test cases for the following:

        {code_or_requirement}

        Please provide comprehensive test cases:

        ```python
        """

        test_cases = generate_response(test_prompt, max_length=1024)

        return jsonify({
            'success': True,
            'input': code_or_requirement,
            'framework': framework,
            'test_cases': test_cases
        })

    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/summarize-code', methods=['POST'])
def summarize_code():
    """Scenario 5: Code Summarizer"""
    try:
        data = request.get_json()
        code = data.get('code', '')

        summary_prompt = f"""
        Analyze and summarize the following code. Explain what it does, its purpose, and key functionality:

        Code:
        ```
        {code}
        ```

        Summary:
        """

        summary = generate_response(summary_prompt, max_length=512)

        return jsonify({
            'success': True,
            'code': code,
            'summary': summary
        })

    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/chat', methods=['POST'])
def chat():
    """Scenario 6: Floating AI Chatbot Assistant"""
    try:
        data = request.get_json()
        message = data.get('message', '')

        chat_prompt = f"""
        You are an AI assistant specialized in Software Development Lifecycle (SDLC).
        Answer the following question helpfully and concisely:

        Question: {message}

        Answer:
        """

        response = generate_response(chat_prompt, max_length=512)

        return jsonify({
            'success': True,
            'message': message,
            'response': response
        })

    except Exception as e:
        return jsonify({'error': str(e)}), 500

def start_server():
    """Start the Flask server"""
    app.run(host='0.0.0.0', port=5000, debug=False, use_reloader=False)

# Main execution
if __name__ == '__main__':
    print("Setting up SmartSDLC Backend...")

    # Setup GPU and model
    setup_gpu_and_model()

    # Start ngrok tunnel for external access
    ngrok.set_auth_token("AUTH_TOKEN")  # Replace with your ngrok token
    public_url = ngrok.connect(5000)
    print(f"Public URL: {public_url}")

    # Start Flask server in a separate thread
    server_thread = threading.Thread(target=start_server)
    server_thread.daemon = True
    server_thread.start()

    print("SmartSDLC Backend is running!")
    print(f"Local URL: http://localhost:5000")
    print(f"Public URL: {public_url}")

    # Keep the main thread alive
    try:
        while True:
            time.sleep(1)
    except KeyboardInterrupt:
        print("Shutting down...")